{
  "recipes": {
  },
  "groupings": {
  },
  "recommendations": {
  },
  "description": "Installs hadoop and sets up a high-performance cluster. Inspired by Tom White / Cloudera's hadoop-ec2 command line utilities",
  "suggestions": {
  },
  "attributes": {
  },
  "conflicting": {
  },
  "dependencies": {
    "ebs": [

    ],
    "cluster_service_discovery": [

    ],
    "java": [

    ],
    "aws": [

    ],
    "ubuntu": [

    ]
  },
  "long_description": "= DESCRIPTION:\n\nInstalls Apache hadoop and sets up a basic distributed cluster per the quick start documentation.\n\n= REQUIREMENTS:\n\n== Platform:\n\nDesigned to work on the Amazon cloud using EBS-backed instances, though many parts of it will work elsewhere.\n\n== Cookbooks:\n\nOpscode cookbooks, http://github.com/opscode/cookbooks/tree/master:\n\n* java\n\n= ATTRIBUTES: \n\n* Hadoop package/version info:\n** hadoop[:hadoop_handle] - Specify the package version of hadoop to install. Default hadoop-0.20\n** hadoop[:cdh_version]   - Specify the cloudera distribution version. Default is cdh3 -- see http://archive.cloudera.com/docs/_apt.html\n* You'll need to grab your AWS credentials from somewhere and stuff them in: \n** aws[:aws_access_key]\n** aws[:aws_secret_access_key]\n* See the corresponding entries in the hadoop documentation for the following:\n** hadoop[:dfs_replication]           \n** hadoop[:disks_to_prep].inspect     \n** hadoop[:mapred_local_dirs]         \n** hadoop[:max_map_tasks]             \n** hadoop[:max_reduce_tasks]          \n** hadoop[:cluster_reduce_tasks]      \n** hadoop[:java_child_opts]           \n** hadoop[:java_child_ulimit]         \n** hadoop[:dfs_name_dirs]             \n** hadoop[:fs_checkpoint_dirs]        \n** hadoop[:dfs_data_dirs]             \n\nYou may wish to add more attributes for tuning the configuration file templates.\n\n= DATABAGS:\n\nYou must construct a databag named \"servers_info\" containing the addresses\nfor the various central nodes. If your hadoop cluster is named 'zaius'\nyou'll set\n\n  {\"id\":\"zaius_namenode\",  \"private_ip\":\"10.212.171.245\"}\n  {\"id\":\"zaius_jobtracker\",\"private_ip\":\"10.212.171.245\"}\n\n\n= USAGE:\n\nThis cookbook installs hadoop from the cloudera CDH3 distribution[1] . You should copy this to a site-cookbook and modify the templates to meet your requirements. \n\nThe various hadoop processes are installed as services. Do NOT use the start-all.sh scripts.  \n\nThe recipes correspond to different roles you'll probably assign: \n* pseudo-conf       -- single machine pseudo-distributed mode\n* jobtracker        -- assigns and coordinates jobs\n* namenode          -- runs the namenode (coordinates the HDFS) and secondarynamenode (backs up the metadata file)\n* worker            -- runs the datanode and tasktracker\n* secondarynamenode -- additional secondarynamenode (backs up the metadata file).\n\nIn the roles/ directory at http://github.com/mrflip/hadoop_cluster_chef there are defined chef roles for generic hadoop node, hadoop master (job, name, web, secondaryname services), and hadoop worker (data and task services)\n\nAssign node roles according to these rough guidelines:\n\n* For initial testing, use pseudo-conf mode.\n* For clusters of some to a dozen or so nodes, give the master node the jobtracker, namenode *and* worker roles.\n* For larger clusters, omit the worker role for the master node.\n* For huge clusters, run the jobtracker and namenode/secondarynamenode on different hosts.\n\nNote that the secondarynamenode is NOT a redundant namenode. All it does is make periodic backups of the HDFS metadata.\n\n[1] http://archive.cloudera.com/docs/\n\n= LICENSE and AUTHOR:\n      \nAuthor:: Joshua Timberman (<joshua@opscode.com>), Flip Kromer (<flip@infochimps.org>), much code taken from Tom White (<tom@cloudera.com>)'s hadoop-ec2 scripts and Robert Berger (http://blog.ibd.com)'s blog posts.\n\nCopyright:: 2009, Opscode, Inc\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n",
  "providing": {
  },
  "platforms": {
    "debian": [

    ],
    "ubuntu": [

    ]
  },
  "version": "0.9.6",
  "license": "Apache 2.0",
  "maintainer": "Infochimps.org",
  "name": "hadoop_cluster",
  "replacing": {
  },
  "maintainer_email": "help@infochimps.org"
}